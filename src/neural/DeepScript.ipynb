{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepScript.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"text","id":"157DU1t2Z5Yt"},"cell_type":"markdown","source":["# **Uploading Files onto Google Colab**\n","\n","Word Embeddings have been uploaded on Google Drive in a folder called **Word2Vec**.\n","\n","They were trained by the Gensim module with dimensions of both 200 and 300, window size of 5, and 500 iterations.\n","\n","Here, we will access those files. \n","\n","Note: The URL for the Word2Vec folder in my Google Drive is : https://drive.google.com/drive/folders/1sdDeXX3XTdJg5tEnQNhmjNmol7DPzOZH\n","\n","That is why I set the the q paramter is set to: **1sdDeXX3XTdJg5tEnQNhmjNmol7DPzOZH**\n","\n","You will want to make a copy of the Word2Vec folder and put it in your Google Drive's 'Colab Notebooks' folder. Then you will want to change the q parameter to the end of the URL of the Word2Vec folder."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1543363606577,"user_tz":300,"elapsed":14410,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"id":"4ufZnIqP_GWV","outputId":"98655d92-9935-43dd-936b-6ff89e25b434","colab":{"base_uri":"https://localhost:8080/","height":344}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","local_download_path = os.path.expanduser('~/Word2Vec')\n","try:\n","  os.makedirs(local_download_path)\n","except: pass\n","\n","file_list = drive.ListFile(\n","    {'q': \"'1sdDeXX3XTdJg5tEnQNhmjNmol7DPzOZH' in parents\"}).GetList()\n","\n","for f in file_list:\n","  # 3. Create & download by id.\n","  print('title: %s, id: %s' % (f['title'], f['id']))\n","  fname = os.path.join(local_download_path, f['title'])\n","  print('downloading to {}'.format(fname))\n","  f_ = drive.CreateFile({'id': f['id']})\n","  f_.GetContentFile(fname)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["title: Bad Sentences.csv, id: 1zcZm3griSFElsa2te1Hhd5lpAl7tl5vI\n","downloading to /root/Word2Vec/Bad Sentences.csv\n","title: Good Sentences.csv, id: 1dSk6Zq20YnP51fQqqjR65eERDaFRSrdb\n","downloading to /root/Word2Vec/Good Sentences.csv\n","title: Tokenized Bad Sentences.csv, id: 1n5XEE1aHdoAfBxrqXWn-yEeEjp2WUj1B\n","downloading to /root/Word2Vec/Tokenized Bad Sentences.csv\n","title: word2vec_Good_300, id: 1zWEoKRHFRToBzqkLLvA00N2HBcDvG_4F\n","downloading to /root/Word2Vec/word2vec_Good_300\n","title: word2vec_Good_200, id: 1NT7o_yI1IAHfGFcw9Wrq3CKNeC9md4T2\n","downloading to /root/Word2Vec/word2vec_Good_200\n","title: word2vec_Bad_300, id: 1zv6AxhipBjhD1rwID1HTE5zncVJPVGms\n","downloading to /root/Word2Vec/word2vec_Bad_300\n","title: word2vec_Bad_200, id: 1Giw1gcBYoncfYMGi6LKtEJIeYZ_a2PcC\n","downloading to /root/Word2Vec/word2vec_Bad_200\n","title: Tokenized Good Sentences.csv, id: 1YPAgpNciFdTAKQUsrcRs0RLaivevlNoT\n","downloading to /root/Word2Vec/Tokenized Good Sentences.csv\n","title: Tokenized Bad Sentences.csv, id: 1S8P8EC47oT46b7ql-xht2QT0rqMCy0yA\n","downloading to /root/Word2Vec/Tokenized Bad Sentences.csv\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"uJT1SxtinnjM"},"cell_type":"markdown","source":["# Importing Tokenized Sentences\n","\n","I have also uploaded two csv files called **Tokenized Bad Sentences.csv** and **Tokenized Good Sentences.csv**  found in the Word2Vec folder. \n","\n","Below is the code to read in those files."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1543363611837,"user_tz":300,"elapsed":830,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"id":"xfdxBIsSkjuk","outputId":"a9fd4956-4113-444c-ce95-243db329e142","colab":{"base_uri":"https://localhost:8080/","height":73}},"cell_type":"code","source":["import csv\n","with open('/root/Word2Vec/Bad Sentences.csv', 'r', encoding=\"utf-8\") as f:\n","    reader = csv.reader(f)\n","    badSentences = list(reader)\n","    \n","with open('/root/Word2Vec/Good Sentences.csv', 'r', encoding=\"utf-8\") as f:\n","    reader = csv.reader(f)\n","    goodSentences = list(reader)\n","    \n","print(badSentences[0:10])   \n","print(goodSentences[0:10])  "],"execution_count":2,"outputs":[{"output_type":"stream","text":["[['oh', ',', 'shit', '.'], ['you', 'just', 'got', 'wolfed', '.'], ['what', '?'], ['that', 'is', 'an', 'official', 'trademark', 'that', 'i', 'am', 'getting', 'registered', '.'], ['it', \"'s\", 'a', 'lot', 'of', 'stuff', 'you', 'got', 'ta', 'do', ',', 'hoops', 'you', 'got', 'ta', 'jump', 'through', '.'], ['got', 'ta', 'get', 'on', 'the', 'internet', '.'], ['got', 'ta', 'go', 'to', 'some', 'stupidass', 'website', 'where', 'you', 'register', 'a', 'catch', 'phrase', '.'], ['i', 'wanted', '``', 'bam', ',', \"''\", 'but', 'emeril', 'had', 'taken', 'it', '.'], ['i', \"'m\", 'rambling', ',', 'man', '.'], ['get', 'up', ',', 'man', '.']]\n","[['mr.', 'dufresne', ',', 'describe', 'the', 'confrontation', 'you', 'had', 'with', 'your', 'wife', 'the', 'night', 'she', 'was', 'murdered', '.'], ['it', 'was', 'very', 'bitter', '.'], ['she', 'said', 'she', 'was', 'glad', 'i', 'knew', ',', 'that', 'she', 'hated', 'all', 'the', 'sneaking', 'around', '.'], ['and', 'she', 'said', 'that', 'she', 'wanted', 'a', 'divorce', 'in', 'reno', '.'], ['what', 'was', 'your', 'response', '?'], ['i', 'told', 'her', 'i', 'would', 'not', 'grant', 'one', '.'], ['``', 'i', \"'ll\", 'see', 'you', 'in', 'hell', 'before', 'i', 'see', 'you', 'in', 'reno', '.'], [\"''\", 'those', 'were', 'your', 'words', ',', 'according', 'to', 'your', 'neighbors', '.'], ['if', 'they', 'say', 'so', '.'], ['i', 'really', 'do', \"n't\", 'remember', '.']]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"YmIy6TDWaJ0k"},"cell_type":"markdown","source":["# **Installing gensim**\n","\n","Gensim is the Python module used to train the word2vec embeddings. Here is how to upload the files."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1543363627929,"user_tz":300,"elapsed":11378,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"id":"cfn6Z8SxXu4W","outputId":"8e93648f-74e0-4f0e-a8ad-151630b14d29","colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["!pip install -q gensim\n","from gensim.models import Word2Vec\n","model_Bad = Word2Vec.load(\"/root/Word2Vec/word2vec_Bad_300\")\n","model_Good = Word2Vec.load(\"/root/Word2Vec/word2vec_Good_300\")\n","print(model_Bad)\n","print(model_Good)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Word2Vec(vocab=29504, size=300, alpha=0.025)\n","Word2Vec(vocab=30426, size=300, alpha=0.025)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"qUR_r-pdaQRD"},"cell_type":"markdown","source":["# **Similar Vectors**\n","\n","Once the word2vec embeddings are uploaded, you can view the vectors most similar to a given word. "]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1543363630631,"user_tz":300,"elapsed":412,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"id":"D33t0UbuZaHI","outputId":"95ed5910-6110-40b2-cf30-cc6848db1a19","colab":{"base_uri":"https://localhost:8080/","height":455}},"cell_type":"code","source":["for i in model_Bad.wv.most_similar (positive = 'good'):\n","  print(i)\n","  \n","print()\n","\n","for i in model_Good.wv.most_similar (positive = 'good'):\n","  print(i)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["('bad', 0.42860960960388184)\n","('nice', 0.40062108635902405)\n","('like', 0.3491782248020172)\n","('tough', 0.3429213762283325)\n","('great', 0.34254634380340576)\n","('better', 0.28865498304367065)\n","('big', 0.28429123759269714)\n","('weird', 0.2701437175273895)\n","('happy', 0.26822713017463684)\n","('hard', 0.26718002557754517)\n","\n","('bad', 0.5157124996185303)\n","('nice', 0.42234158515930176)\n","('great', 0.3885391056537628)\n","('smart', 0.3636835813522339)\n","('fine', 0.3554360568523407)\n","('tough', 0.34922707080841064)\n","('big', 0.3384079933166504)\n","('funny', 0.33056941628456116)\n","('hard', 0.3305012583732605)\n","('tempting', 0.31914812326431274)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"NQJC4RcYfTQw"},"cell_type":"markdown","source":["# **Word2Vec Weights onto Keras**\n","\n","Because we are going to use Keras to train an RNN, here is how to extract the actual pretrained weights of the word embedding which can be used for the neural network."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1543363634580,"user_tz":300,"elapsed":1549,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"id":"ouLu8Z-Ja4x6","outputId":"c8137c89-69cf-491f-a86b-a59fdef891bc","colab":{"base_uri":"https://localhost:8080/","height":271}},"cell_type":"code","source":["from keras.layers import Embedding\n","\n","pretrained_weights_Bad = model_Bad.wv.vectors \n","pretrained_weights_Good = model_Good.wv.vectors\n","\n","embeddingBad = Embedding(input_dim=pretrained_weights_Bad.shape[0], output_dim=pretrained_weights_Bad.shape[1], \n","                    weights=[pretrained_weights_Bad])\n","\n","embeddingGood = Embedding(input_dim=pretrained_weights_Good.shape[0], output_dim=pretrained_weights_Good.shape[1], \n","                    weights=[pretrained_weights_Good])\n","\n","print(pretrained_weights_Bad)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[[ 1.25372    -1.2052641  -0.25990063 ... -0.976092    1.8551047\n","   1.3307298 ]\n"," [ 1.4823085  -1.2304381  -0.9231364  ... -1.6590116   1.3164423\n","   0.8863562 ]\n"," [ 2.0804706  -0.7694774   0.17020172 ... -0.5701194   1.6237695\n","   1.2792978 ]\n"," ...\n"," [ 0.6441616   0.12467387  0.17375445 ...  0.27845207  0.394679\n","  -0.18595086]\n"," [ 0.70862037  0.14693536  0.24079292 ...  0.2610408   0.41543\n","  -0.17618927]\n"," [-0.08563908  0.16787037 -0.30501494 ...  0.32231143  0.14400984\n","   0.1407564 ]]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"GwiBPHfmqBAA"},"cell_type":"markdown","source":["# LSTM Neural Network\n","\n","Here, we design the architecture for the neural network. You will want to tinker with this to get something that trains in a reasonable number of time, but has good performance."]},{"metadata":{"colab_type":"code","id":"SKGPGSQTCl5C","colab":{"base_uri":"https://localhost:8080/","height":671},"outputId":"6fc324a5-b50f-4342-dff6-cbfaabd1cf18","executionInfo":{"status":"ok","timestamp":1543363639767,"user_tz":300,"elapsed":3087,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Activation, Dense, Bidirectional, Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","vocab_size, emdedding_size = pretrained_weights_Bad.shape\n","\n","# This is where you define the models for the bad movie neural net, and the good movie neural net.\n","# It is important that the models are seperate so you don't fit the model to both datasets.\n","# MAKE SURE BOTH MODELS HAVE THE SAME PARAMETERS\n","def get_bad_movie_model():\n","  model = Sequential()\n","  model.add(embeddingBad)\n","  model.add(Bidirectional(LSTM(units=128))) # If you want a non-bidirectional LSTM, just remove the Bidirectional().\n","                                            # It significantly increases the training time, especially if you increase layers/units\n","  model.add(Dropout(rate=0.5)) # Kind of high, but important to avoid overfitting.\n","  model.add(Dense(units=vocab_size))\n","  model.add(Activation('softmax'))\n","  print(model.summary())\n","  return model\n","\n","\n","def get_good_movie_model():\n","  model = Sequential()\n","  model.add(embeddingGood)\n","  model.add(Bidirectional(LSTM(units=128)))\n","  model.add(Dropout(rate=0.5))\n","  model.add(Dense(units=vocab_size))\n","  model.add(Activation('softmax'))\n","  print(model.summary())\n","  return model\n","\n","bad_model = get_bad_movie_model()\n","good_model = get_good_movie_model()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, None, 300)         8851200   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 256)               439296    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 29504)             7582528   \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 29504)             0         \n","=================================================================\n","Total params: 16,873,024\n","Trainable params: 16,873,024\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, None, 300)         9127800   \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 256)               439296    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 29504)             7582528   \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 29504)             0         \n","=================================================================\n","Total params: 17,149,624\n","Trainable params: 17,149,624\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"metadata":{"id":"_F9vn1H6luee","colab_type":"text"},"cell_type":"markdown","source":["## Splitting the Training and Test Sets"]},{"metadata":{"colab_type":"code","id":"5Se5ALUkdwJd","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","train_x = np.zeros([(int)(0.8*len(badSentences)), 1040], dtype=np.int32)\n","train_y = np.zeros([(int)(0.8*len(badSentences))], dtype=np.int32)\n","test_x = np.zeros([len(badSentences) - (int)(0.8*len(badSentences)), 1040], dtype=np.int32)\n","test_y = np.zeros([len(badSentences) - (int)(0.8*len(badSentences))], dtype=np.int32)\n","\n","train_x_good = np.zeros([(int)(0.8*len(goodSentences)), 1040], dtype=np.int32)\n","train_y_good = np.zeros([(int)(0.8*len(goodSentences))], dtype=np.int32)\n","test_x_good = np.zeros([len(goodSentences) - (int)(0.8*len(goodSentences)), 1040], dtype=np.int32)\n","test_y_good = np.zeros([len(goodSentences) - (int)(0.8*len(goodSentences))], dtype=np.int32)\n","\n","np.random.seed = 42 # Seed needs to be for consistent results, as otherwise the training/test set change every run.\n","idc_bad = np.random.permutation(len(badSentences))\n","train_bad, test_bad = idc_bad[:(int)(0.8*len(idc_bad))], idc_bad[(int)(0.8*len(idc_bad)):]\n","idc_good = np.random.permutation(len(goodSentences))\n","train_good, test_good = idc_good[:(int)(0.8*len(idc_good))], idc_good[(int)(0.8*len(idc_good)):]\n","\n","\n","for i, j in enumerate(train_bad):\n","  for t, word in enumerate(badSentences[j][:-1]):\n","    train_x[i, t] = model_Bad.wv.vocab[word].index\n","  train_y[i] = model_Bad.wv.vocab[badSentences[j][-1]].index\n","\n","for i, j in enumerate(test_bad):\n","  for t, word in enumerate(badSentences[j][:-1]):\n","    test_x[i, t] = model_Bad.wv.vocab[word].index\n","  test_y[i] = model_Bad.wv.vocab[badSentences[j][-1]].index\n","\n","  \n","for i, j in enumerate(train_good):\n","  for t, word in enumerate(goodSentences[j][:-1]):\n","    train_x_good[i, t] = model_Good.wv.vocab[word].index\n","  train_y_good[i] = model_Good.wv.vocab[goodSentences[j][-1]].index\n","\n","for i, j in enumerate(test_good):\n","  for t, word in enumerate(goodSentences[j][:-1]):\n","    test_x_good[i, t] = model_Good.wv.vocab[word].index\n","  test_y_good[i] = model_Good.wv.vocab[goodSentences[j][-1]].index\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YUMlFcqmWGW","colab_type":"text"},"cell_type":"markdown","source":["## Callbacks\n","\n","This cell sets up the callbacks for early stopping and saving checkpoints to Google Drive. Makes use of the callback implemented in [this](https://github.com/Zahlii/colab-tf-utils) repository, as due to the way Google Colab works, there is not a way to natively save model checkpoints during training and be able to retrieve them later."]},{"metadata":{"id":"N3VlAd8o9L6V","colab_type":"code","outputId":"25073403-d049-48de-b481-9f112de3c4e2","executionInfo":{"status":"ok","timestamp":1543363688253,"user_tz":300,"elapsed":22962,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"colab":{"base_uri":"https://localhost:8080/","height":689}},"cell_type":"code","source":["!wget https://raw.githubusercontent.com/Zahlii/colab-tf-utils/master/utils.py\n","import utils\n","import os\n","import keras\n","\n","def compare(best, new):\n","  if not best.losses['val_acc']:\n","    print(\"Not best\")\n","  if not new.losses['val_acc']:\n","    print(\"Not new\")\n","  return best.losses['val_acc'] < new.losses['val_acc']\n","\n","def path_b(new):\n","  if new.losses['val_acc'] > 0.65:\n","    return 'bad_movie_model_%s.h5' % new.losses['val_acc']\n","\n","def path_g(new):\n","    if new.losses['val_acc'] > 0.65:\n","        return 'good_movie_model_%s.h5' % new.losses['val_acc']\n","\n","early_stop_b = EarlyStopping(monitor='val_acc', patience=5, verbose=1)\n","early_stop_g = EarlyStopping(monitor='val_acc', patience=5, verbose=1)\n","\n","callbacks_b = cb_b = [\n","    utils.GDriveCheckpointer(compare,path_b),\n","    keras.callbacks.TensorBoard(log_dir=os.path.join(utils.LOG_DIR,'bad_movie_model')),\n","    early_stop_b\n","]\n","\n","callbacks_g = cb_g = [\n","    utils.GDriveCheckpointer(compare,path_g),\n","    keras.callbacks.TensorBoard(log_dir=os.path.join(utils.LOG_DIR,'good_movie_model')),\n","    early_stop_g\n","]"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2018-11-28 00:07:45--  https://raw.githubusercontent.com/Zahlii/colab-tf-utils/master/utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6935 (6.8K) [text/plain]\n","Saving to: ‘utils.py’\n","\n","\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   6.77K  --.-KB/s    in 0s      \n","\n","2018-11-28 00:07:45 (62.5 MB/s) - ‘utils.py’ saved [6935/6935]\n","\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n","rm: cannot remove 'tboard.py': No such file or directory\n","--2018-11-28 00:07:51--  https://raw.githubusercontent.com/mixuala/colab_utils/master/tboard.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5214 (5.1K) [text/plain]\n","Saving to: ‘tboard.py’\n","\n","tboard.py           100%[===================>]   5.09K  --.-KB/s    in 0s      \n","\n","2018-11-28 00:07:51 (59.8 MB/s) - ‘tboard.py’ saved [5214/5214]\n","\n","calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n","calling unzip ngrok-stable-linux-amd64.zip ...\n","ngrok installed. path=/content/ngrok\n","status: tensorboard=False, ngrok=False\n","tensorboard url= http://b4f7eca3.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"OQrPtHkBp23X","colab_type":"text"},"cell_type":"markdown","source":["## Training the Neural Network\n","\n","\n","###Note\n","For loading checkpoints, you will want to get the file ID of the file you want to download. For example, if my checkpoint's url is https://drive.google.com/file/d/1oKxyAyd5fX6dgpsSJeghzy2m5iWu-vQZ/view\n","\n","The id is **1oKxyAyd5fX6dgpsSJeghzy2m5iWu-vQZ**\n","\n","To get the URL for the specific file, you just need to right click it and select 'Get Shareable Link'"]},{"metadata":{"id":"U1kwiIYtG44W","colab_type":"code","outputId":"a19ec074-415d-4cca-ed26-aca6a26232dc","executionInfo":{"status":"error","timestamp":1543364878381,"user_tz":300,"elapsed":17319,"user":{"displayName":"Leonardo Costa","photoUrl":"","userId":"02116991383253551950"}},"colab":{"base_uri":"https://localhost:8080/","height":954}},"cell_type":"code","source":["# This is needed to make sure we are still authenticated and don't throw an Exception when we try to upload/download to Drive\n","from google.colab import files\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","from keras.models import load_model\n","\n","# CHANGE THESE IF YOU ARE RESUMING TRAINING FROM A CERTAIN EPOCH.\n","BAD_MOVIE_INIT_EPOCH = 0\n","GOOD_MOVIE_INIT_EPOCH = 0\n","\n","# CODE BLOCK FOR IF YOU ARE TRAINING THE MODEL FROM SCRATCH\n","# COMMENT OUT IF YOU ARE LOADING A MODEL FROM A CHECKPOINT\n","####################################################################\n","# compile model\n","good_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","bad_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","#####################################################################\n","\n","\n","# CODE BLOCK FOR LOADING THE MODEL FROM AN EXISTING SAVED MODEL\n","# UNCOMMENT THIS IF YOU ARE LOADING A MODEL FROM A CHECKPOINT TO CONTINUE TRAINING\n","###################################################################\n","# drive_chk_bad = drive.CreateFile({'id': '1oKxyAyd5fX6dgpsSJeghzy2m5iWu-vQZ'})\n","# drive_chk_bad.GetContentFile('chkpt_bad.h5')\n","# drive_chk_good = drive.CreateFile({'id': 'FILE_ID_FOR_GOOD_MOVIE_CHKPT'})\n","# drive_chk_good.GetContentFile('chkpt_good.h5')\n","\n","# bad_model = load_model('chkpt_bad.h5')\n","# good_model = load_model ('chkpt_good.h5')\n","###################################################################\n","\n","\n","# fit model\n","bad_model.fit(train_x, train_y, validation_split = 0.2, batch_size=128, epochs=50, callbacks=cb_b,\n","             initial_epoch = BAD_MOVIE_INIT_EPOCH) \n","# save the model to file\n","bad_model.save('bad_movie_model.h5')\n","\n","# files.download('bad_movie_model.h5')\n","# Uncomment if you want it to download the model to your local machine after training\n","b_file = drive.CreateFile()\n","b_file.SetContentFile('bad_movie_model.h5')\n","b_file.Upload()\n","\n","\n","#repeat but for good movies\n","good_model.fit(train_x_good, train_y_good, validation_split = 0.2, batch_size=128, epochs=50, callbacks=cb_g,\n","              initial_epoch = GOOD_MOVIE_INIT_EPOCH)\n","good_model.save('good_movie_model.h5')\n","\n","# files.download('good_movie_model.h5')\n","# Uncomment if you want to download the model to your local machine after training\n","g_file = drive.CreateFile()\n","g_file.SetContentFile('good_movie_model.h5')\n","g_file.Upload()\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Train on 58953 samples, validate on 14739 samples\n","Epoch 8/50\n","  256/58953 [..............................] - ETA: 37:29 - loss: 0.4385 - acc: 0.8086"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-ce89abfbd664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m bad_model.fit(train_x, train_y, validation_split = 0.2, batch_size=128, epochs=50, callbacks=cb_b,\n\u001b[0;32m---> 37\u001b[0;31m              initial_epoch = BAD_MOVIE_INIT_EPOCH) \n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m# save the model to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mbad_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bad_movie_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"lK15aKQicVb3","colab_type":"code","colab":{}},"cell_type":"code","source":["#"],"execution_count":0,"outputs":[]}]}