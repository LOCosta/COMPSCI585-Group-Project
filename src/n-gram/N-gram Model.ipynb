{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokenized Words for Bad Movies: 806955\n",
      "Number of sentences for Bad Movies: 92115\n",
      "Vocab Size: 29506\n",
      "Number of Tokenized Words for Good Movies: 1119560\n",
      "Number of sentences for Good Movies: 125230\n",
      "Vocab Size: 30427\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "import os\n",
    "\n",
    "# Get the number format such as: 001, 002, 010, 100\n",
    "def getNum(i):\n",
    "    num = \"\"\n",
    "    if len(str(i)) == 1:\n",
    "        num = \"00\" + str(i)\n",
    "    if len(str(i)) == 2:\n",
    "        num = \"0\" + str(i)\n",
    "    if len(str(i)) == 3:\n",
    "        num = str(i)\n",
    "    return num\n",
    "\n",
    "# Return a tokensized text of the Transcripts\n",
    "def getMovieTranscripts(low = True):\n",
    "    movieScripts = []\n",
    "    movieScores = \"\"\n",
    "    if low == True:\n",
    "        movieScores = \"Bad\"\n",
    "    else:\n",
    "        movieScores = \"Good\"\n",
    "    #Taking advantage of the naming convension for the files, iterate through the files.\n",
    "    for i in list(range(1,101)):\n",
    "        #Check if the file exists.\n",
    "        if os.path.isfile('data\\\\Processed'+ movieScores + 'Scripts\\\\' + getNum(i) + '-transcript.txt'): \n",
    "            f = open('data\\\\Processed'+ movieScores + 'Scripts\\\\' + getNum(i) + '-transcript.txt', 'r')\n",
    "            content = f.read()\n",
    "            # Normalize the whitespaces and lowercase everything\n",
    "            newContent = \" \".join(content.lower().split())\n",
    "            movieScripts += nltk.word_tokenize(newContent)\n",
    "    return addStartTokens(movieScripts)\n",
    "\n",
    "# Adds a Start Token before each sentence. \n",
    "def addStartTokens(script):\n",
    "    sentenceList = []\n",
    "    sentence = ['[START]']\n",
    "    newList = ['[START]']\n",
    "    count = 1\n",
    "    for token in script:\n",
    "        newList.append(token)\n",
    "        sentence.append(token)  \n",
    "        if token in ['.', '!', '?']:\n",
    "            newList.insert(count + 1, '[START]')\n",
    "            sentenceList.append(sentence)\n",
    "            sentence = ['[START]']\n",
    "            count += 1\n",
    "        count += 1\n",
    "    if newList[-1] == '[START]':\n",
    "        newList.pop()\n",
    "    return newList, sentenceList\n",
    "\n",
    "badMovieScripts, badSentenceList = getMovieTranscripts(low = True)\n",
    "print(\"Number of Tokenized Words for Bad Movies:\", len(badMovieScripts))\n",
    "print(\"Number of sentences for Bad Movies:\", len(badSentenceList))\n",
    "print(\"Vocab Size:\", len(set(badMovieScripts)))\n",
    "\n",
    "goodMovieScripts, goodSentenceList = getMovieTranscripts(low = False)\n",
    "print(\"Number of Tokenized Words for Good Movies:\", len(goodMovieScripts))\n",
    "print(\"Number of sentences for Good Movies:\", len(goodSentenceList))\n",
    "print(\"Vocab Size:\", len(set(goodMovieScripts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Dictionary\n",
    "\n",
    "Create a dictionary such that calling dictionary['[START]'] returns a dictionary of all of the words that came after it along with its counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] oh 1893\n",
      "[START] you 4665\n",
      "[START] what 2739\n",
      "[START] that 1480\n",
      "[START] it 2441\n",
      "[START] got 97\n",
      "[START] i 9058\n",
      "[START] get 557\n",
      "[START] is 375\n",
      "[START] ready 31\n",
      "[START] three 35\n",
      "[START] things 18\n",
      "[START] whoa 134\n",
      "[START] u.s. 3\n",
      "[START] they 809\n",
      "[START] a 673\n",
      "[START] amy 13\n",
      "[START] piss 2\n",
      "[START] listen 163\n",
      "[START] when 187\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def createBigramCount(scripts):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    bigrams = ngrams(scripts, 2)\n",
    "    for i in scripts:\n",
    "        for word1, word2 in bigrams:\n",
    "            model[word1][word2] += 1\n",
    "    return model\n",
    "\n",
    "model = createBigramCount(badMovieScripts)\n",
    "#model['[START]']\n",
    "\n",
    "#The dictionary is structured like this: [word1 : [word2: Count(word2)]]\n",
    "count = 0\n",
    "for word2 in model['[START]']:\n",
    "    if count < 20:\n",
    "        print('[START]', word2, model['[START]'][word2])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model\n",
    "\n",
    "Given the movies script, the starting word, number of sentences, and alpha value, generate its own sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bigramPredict(scripts, word, length):\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    sentence = word.split()\n",
    "    #print(np.random.choice(list(model[sentence[-1]]), 1, p = [float(i)/sum(model[sentence[-1]].values()) for i in model[sentence[-1]].values()]))\n",
    "    count = 0\n",
    "    # Punctuations will be used to determine the ending of a sentence.\n",
    "    punctuations = ['.', '!', '?']\n",
    "    while count < length:\n",
    "        randomToken = max(bigramModel[sentence[-1]], key=bigramModel[sentence[-1]].get)  \n",
    "        #randomToken = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i + alpha)/(sum(bigramModel[sentence[-1]].values()) + (alpha * len(bigramModel[sentence[-1]].keys()))) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "        #randomToken = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i)/(sum(bigramModel[sentence[-1]].values())) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "        if randomToken in punctuations:\n",
    "            count += 1\n",
    "        sentence.append(randomToken)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Dictionary\n",
    "\n",
    "Create a dictionary such that calling dictionary['[START]', 'oh'] returns a dictionary of all of the words that came after these two pairs along with its counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest count after 'you, just' is:  ,\n",
      "('[START]', 'oh') , 1415\n",
      "('[START]', 'oh') no 5\n",
      "('[START]', 'oh') my 46\n",
      "('[START]', 'oh') how 1\n",
      "('[START]', 'oh') god 11\n",
      "('[START]', 'oh') . 140\n",
      "('[START]', 'oh') yeah 19\n",
      "('[START]', 'oh') right 2\n",
      "('[START]', 'oh') dropped 1\n",
      "('[START]', 'oh') on 1\n",
      "('[START]', 'oh') well 3\n",
      "('[START]', 'oh') coopie 1\n",
      "('[START]', 'oh') hey 1\n",
      "('[START]', 'oh') and 2\n",
      "('[START]', 'oh') hi 3\n",
      "('[START]', 'oh') listen 1\n",
      "('[START]', 'oh') really 1\n",
      "('[START]', 'oh') look 2\n",
      "('[START]', 'oh') shit 6\n",
      "('[START]', 'oh') fuck 1\n",
      "('[START]', 'oh') ! 172\n",
      "('[START]', 'oh') harsh 1\n",
      "('[START]', 'oh') ohh 1\n",
      "('[START]', 'oh') all 1\n",
      "('[START]', 'oh') that 1\n",
      "('[START]', 'oh') simon 1\n",
      "('[START]', 'oh') ugly 1\n",
      "('[START]', 'oh') : 1\n",
      "('[START]', 'oh') hello 1\n",
      "('[START]', 'oh') thanks 1\n",
      "('[START]', 'oh') cute 1\n",
      "('[START]', 'oh') oh 3\n",
      "('[START]', 'oh') wait 1\n",
      "('[START]', 'oh') love 1\n",
      "('[START]', 'oh') we 2\n",
      "('[START]', 'oh') were 1\n",
      "('[START]', 'oh') yes 3\n",
      "('[START]', 'oh') now 2\n",
      "('[START]', 'oh') barb 1\n",
      "('[START]', 'oh') ? 5\n",
      "('[START]', 'oh') boy 2\n",
      "('[START]', 'oh') i 4\n",
      "('[START]', 'oh') right.. 1\n",
      "('[START]', 'oh') snap 1\n",
      "('[START]', 'oh') yea 1\n",
      "('[START]', 'oh') great 2\n",
      "('[START]', 'oh') peter 1\n",
      "('[START]', 'oh') ah 1\n",
      "('[START]', 'oh') what 1\n",
      "('[START]', 'oh') get 1\n",
      "('[START]', 'oh') it 2\n",
      "('[START]', 'oh') very 1\n",
      "('[START]', 'oh') okay 1\n",
      "('[START]', 'oh') ready 1\n",
      "('[START]', 'oh') if 1\n",
      "('[START]', 'oh') dear 1\n",
      "('[START]', 'oh') fred 2\n",
      "('[START]', 'oh') dumdum 1\n",
      "('[START]', 'oh') you 1\n",
      "('[START]', 'oh') ' 1\n",
      "('[START]', 'oh') forget 1\n",
      "('[START]', 'oh') weird 1\n",
      "('[START]', 'oh') thank 1\n"
     ]
    }
   ],
   "source": [
    "def createTrigramCount(scripts):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    trigrams = ngrams(scripts, 3)\n",
    "    for i in scripts:\n",
    "        for word1, word2, word3 in trigrams:\n",
    "            model[(word1, word2)][word3] += 1\n",
    "    return model\n",
    "\n",
    "model = createTrigramCount(badMovieScripts)\n",
    "#The dictionary is structured like this: [(word1, word2) : [word 3: Count(word3)]]\n",
    "word1_word2 = ('[START]', 'oh')\n",
    "print(\"The highest count after 'you, just' is: \", max(model[word1_word2], key=model[word1_word2].get))\n",
    "for word3 in model[word1_word2]:\n",
    "    print(word1_word2, word3, model[word1_word2][word3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Given the movies script, the starting word, number of sentences, and alpha value, generate its own sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def trigramPredict(scripts, word, length):\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    trigramModel = createTrigramCount(scripts)\n",
    "    sentence = []\n",
    "    sentence.append(word)\n",
    "    #sentence.append(np.random.choice(list(bigramModel[word]), 1, p = [float(i + alpha)/(sum(bigramModel[word].values()) + (alpha * len(bigramModel[word].keys()))) for i in bigramModel[word].values()])[0])\n",
    "    #sentence.append(np.random.choice(list(bigramModel[word]), 1, p = [float(i)/(sum(bigramModel[word].values())) for i in bigramModel[word].values()])[0])\n",
    "    sentence.append(max(bigramModel['[START]'], key=bigramModel['[START]'].get))\n",
    "    \n",
    "    count = 0\n",
    "    punctuations = ['.', '!', '?']\n",
    "    while count < length:\n",
    "        if len(trigramModel[(sentence[-2], sentence[-1])]) == 0:\n",
    "            pick = max(bigramModel[sentence[-1]], key=bigramModel[sentence[-1]].get)\n",
    "            #pick = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i + alpha)/(sum(bigramModel[sentence[-1]].values()) + (alpha * len(bigramModel[sentence[-1]].keys()))) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "            #pick = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i)/(sum(bigramModel[sentence[-1]].values())) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "            if pick in punctuations:\n",
    "                count += 1\n",
    "            sentence.append(pick)\n",
    "        else:\n",
    "            maxToken = max(trigramModel[(sentence[-2], sentence[-1])], key=trigramModel[(sentence[-2], sentence[-1])].get)\n",
    "            #randomToken = np.random.choice(list(trigramModel[(sentence[-2], sentence[-1])]), 1, p = [float(i + alpha)/(sum(trigramModel[(sentence[-2], sentence[-1])].values()) + (alpha * len(trigramModel[(sentence[-2], sentence[-1])].keys()))) for i in trigramModel[(sentence[-2], sentence[-1])].values()])[0]\n",
    "            #randomToken = np.random.choice(list(trigramModel[(sentence[-2], sentence[-1])]), 1, p = [float(i)/(sum(trigramModel[(sentence[-2], sentence[-1])].values())) for i in trigramModel[(sentence[-2], sentence[-1])].values()])[0]\n",
    "            if maxToken in punctuations:\n",
    "                count += 1\n",
    "            sentence.append(maxToken)\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sentences\n",
    "\n",
    "To make the sentences presentable, remove the start token, and correct the spacings in between words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Trigram - Bad Movies ###\n",
      "I'm not gon na be a good time.\n",
      "\n",
      "### Trigram - Good Movies ###\n",
      "I'm not going to be a little more.\n"
     ]
    }
   ],
   "source": [
    "def generate(ls):\n",
    "    output = ls[1].capitalize()\n",
    "    for token in ls[2:]:\n",
    "        if token == '[START]':\n",
    "            pass\n",
    "        elif output[-1] in ['.', '!', '?'] or token == 'i':\n",
    "            output += \" \" + token.capitalize()\n",
    "        elif token in [',', ':', '.', '!', '?'] or token[0] == \"'\" or token == \"n't\":\n",
    "            output += token\n",
    "        else:\n",
    "            output += \" \" + token\n",
    "    return output\n",
    "\n",
    "print(\"### Trigram - Bad Movies ###\")\n",
    "print(generate(trigramPredict(badMovieScripts, '[START]', 1)))\n",
    "print()\n",
    "print(\"### Trigram - Good Movies ###\")\n",
    "print(generate(trigramPredict(goodMovieScripts, '[START]', 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Sentences\n",
    "\n",
    "Calculate the probability of a sentence being generated by the n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: Prob( ['[START]', 'you', 'have', 'got', 'superstar', 'written', 'aii', 'over', 'you', '.'] ) =  -5.1787040961277295\n",
      "['[START]', 'you', 'have', 'got', 'superstar', 'written', 'aii', 'over', 'you', '.']\n",
      "w1 -4.303502779419425\n",
      "2 -  -4.9492566264140425\n",
      "3 -  -6.638073837180718\n",
      "4 -  -3.321928094887362\n",
      "5 -  -2.8073549220576046\n",
      "6 -  -3.1699250014423126\n",
      "7 -  -5.149747119504682\n",
      "8 -  -1.0\n",
      "9 -  -0.4150374992788438\n",
      "Trigram: Prob( ['[START]', 'you', 'have', 'got', 'superstar', 'written', 'aii', 'over', 'you', '.'] ) =  9.034736893409807\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def bigramProb(sentence, scripts):\n",
    "    s = sentence\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    prob = 0\n",
    "    index = 0\n",
    "    product = 0\n",
    "    for word in s[1:]:\n",
    "        #print((s[index], word))\n",
    "        if sum(bigramModel[s[index]].values()) != 0:\n",
    "            product = bigramModel[s[index]][word]/sum(bigramModel[s[index]].values())\n",
    "            product = math.log(product, 2)\n",
    "            prob += product\n",
    "        product = 0\n",
    "        index += 1\n",
    "    return prob/len(s)\n",
    "\n",
    "def trigramProb(sentence, scripts):\n",
    "    s = sentence\n",
    "    print(s)\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    trigramModel = createTrigramCount(scripts)\n",
    "    #Always get the start token: log(1) = 0\n",
    "    prob = 0\n",
    "    #P(W2)\n",
    "    if bigramModel[s[0]][s[1]] != 0 and sum(bigramModel[s[0]].values()) != 0:\n",
    "        p = bigramModel[s[0]][s[1]]/sum(bigramModel[s[0]].values())\n",
    "        prob += math.log(p, 2)\n",
    "    print(\"w1\", prob)\n",
    "    #P(W3 - onwards)\n",
    "    index = 2\n",
    "    product = 0\n",
    "    for word in s[2:]:\n",
    "        freq = trigramModel[(s[index-2], s[index -1])][s[index]]\n",
    "        total = sum(trigramModel[(s[index-2], s[index -1])].values())\n",
    "        if freq != 0 and total != 0:\n",
    "            product = freq/total\n",
    "            product = math.log(product, 2)\n",
    "            prob += product\n",
    "        if product == 0 and bigramModel[s[index - 1]][word] != 0 and sum(bigramModel[s[index - 1]].values()) != 0:\n",
    "            product = bigramModel[s[index - 1]][word]/sum(bigramModel[s[index - 1]].values())\n",
    "            product = math.log(product, 2)\n",
    "            prob += product\n",
    "        if product == 0:\n",
    "            product = (1 + 0.1) / (len(bigramModel[s[index - 1]].values()) + (0.1*(len(set(badMovieScripts)))))\n",
    "            product = math.log(product, 2)\n",
    "            prob += product\n",
    "        print(index, \"- \", product)\n",
    "        product = 0\n",
    "        index += 1\n",
    "    return pow(2, -(prob/len(s)))\n",
    "\n",
    "#EXAMPLE\n",
    "s = ['[START]', 'you', 'have', 'got', 'superstar', 'written', 'aii', 'over', 'you', '.']\n",
    "print(\"Bigram: Prob(\", s, \") = \", bigramProb(s, badMovieScripts))\n",
    "print(\"Trigram: Prob(\", s, \") = \", trigramProb(s, badMovieScripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(1)\n",
    "random.shuffle(badSentenceList)\n",
    "random.shuffle(goodSentenceList)\n",
    "\n",
    "trainSet_Bad = badSentenceList[: 82904] # 90%\n",
    "trainSet_Good = goodSentenceList[: 112707] # 90%\n",
    "flattenBad = [sentence for sublist in trainSet_Bad for sentence in sublist]\n",
    "flattenGood = [sentence for sublist in trainSet_Good for sentence in sublist]\n",
    "\n",
    "testSet_Bad = badSentenceList[82904:] # 10%\n",
    "testSet_Good = goodSentenceList[112707:] # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity of Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.3270132684494\n"
     ]
    }
   ],
   "source": [
    "perplexitySentences = []\n",
    "\n",
    "def perplexity(testSet):\n",
    "    bigramModel = createBigramCount(flattenBad)\n",
    "    trigramModel = createTrigramCount(flattenBad)\n",
    "    summation = 0\n",
    "    words = 0\n",
    "    for s in testSet:\n",
    "        # substract 1 to ignore the start token\n",
    "        words += (len(s) - 1)\n",
    "        pSentence = 0\n",
    "        for index, word in enumerate(s):\n",
    "            # P(word | [START])\n",
    "            if index == 1:\n",
    "                p = (bigramModel['[START]'][s[index]] + 0.1)/(sum(bigramModel['[START]'].values()) + (0.1 * len(bigramModel['[START]'].values())))\n",
    "                summation += math.log(p, 2)\n",
    "                pSentence += math.log(p, 2)\n",
    "            elif index > 1:\n",
    "                freq = trigramModel[(s[index-2], s[index -1])][s[index]]\n",
    "                if freq != 0:\n",
    "                    total = sum(trigramModel[(s[index-2], s[index -1])].values())\n",
    "                    p = (freq + 0.1)/(total + (0.1 * len(trigramModel[(s[index-2], s[index -1])].values())))\n",
    "                    summation += math.log(p, 2)\n",
    "                    pSentence += math.log(p, 2)\n",
    "                else:\n",
    "                    # Backoff\n",
    "                    p = (bigramModel[index - 1][s[index]] + 0.1)/(sum(bigramModel[index-1].values()) + (0.1 * len(bigramModel[index-1].values())))\n",
    "                    if p != 0:\n",
    "                        summation += math.log(p, 2)\n",
    "                        pSentence += math.log(p, 2)\n",
    "                    else:\n",
    "                        p = 0.1/(total + (0.1 * len(bigramModel[s[index -1]].values())))\n",
    "                        summation += math.log(p, 2)\n",
    "                        pSentence += math.log(p, 2)\n",
    "        perplexitySentences.append((pSentence, s))\n",
    "    average = summation/words\n",
    "    return pow(2, -average)\n",
    "\n",
    "print(perplexity(testSet_Bad))\n",
    "\n",
    "#Bad: 111.97985098906223     69.3270132684494\n",
    "#Good: 113.56405507656099    77.69581596661679\n",
    "\n",
    "# perplexityList = sorted(perplexitySentences, key=lambda x: x[0])\n",
    "\n",
    "# for i in perplexityList[0:10]:\n",
    "#     print(len(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity of Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.99396460330078\n"
     ]
    }
   ],
   "source": [
    "def perplexityBigram(testSet):\n",
    "    bigramModel = createBigramCount(flattenBad)\n",
    "    summation = 0\n",
    "    words = 0\n",
    "    for s in testSet:\n",
    "        # substract 1 to ignore the start token\n",
    "        words += (len(s) - 1)\n",
    "        for index, word in enumerate(s):\n",
    "            # P(word | [START])\n",
    "            if index > 1:\n",
    "                p = (bigramModel[index - 1][s[index]] + 0.1)/(sum(bigramModel[index - 1].values()) + (0.1 * len(bigramModel[index - 1].values())))\n",
    "                summation += math.log(p, 2)\n",
    "    average = summation/words\n",
    "    return pow(2, -average)\n",
    "\n",
    "print(perplexityBigram(testSet_Bad))\n",
    "\n",
    "#Bad: 111.97985098906223\n",
    "#Good: 113.56405507656099\n",
    "\n",
    "#Bad: 194.99396460330078\n",
    "#Bad: 274.03572856788634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bad Movies:  0.23698533858550858\n",
      "Accuracy of Good Movies:  0.22967978907992878\n"
     ]
    }
   ],
   "source": [
    "def cost(train, test):\n",
    "    match = 0\n",
    "    total = 0\n",
    "    bigramModel = createBigramCount(train)\n",
    "    trigramModel = createTrigramCount(train)\n",
    "\n",
    "    for sentence in test:\n",
    "        #print(sentence)\n",
    "        index = 0\n",
    "        maxToken = ''\n",
    "        for word in range(0, len(sentence) - 2):\n",
    "            wordPair = (sentence[index], sentence[index + 1])\n",
    "            if len(trigramModel[wordPair]) == 0:\n",
    "                if len(bigramModel[sentence[index + 1]]) == 0:\n",
    "                    #print(wordPair)\n",
    "                    maxToken = ''\n",
    "                else:\n",
    "                    maxToken = max(bigramModel[sentence[index + 1]], key=bigramModel[sentence[index + 1]].get)\n",
    "            else:\n",
    "                maxToken = max(trigramModel[wordPair], key=trigramModel[wordPair].get)\n",
    "            #print(maxToken, sentence[index + 2])\n",
    "            if maxToken == sentence[index + 2]:\n",
    "                match += 1\n",
    "            total += 1\n",
    "            index += 1\n",
    "    return match/total\n",
    "        \n",
    "print(\"Accuracy of Bad Movies: \", cost(flattenBad, testSet_Bad))\n",
    "print(\"Accuracy of Good Movies: \", cost(flattenGood, testSet_Good))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bad Movies:  0.14571517301122897\n",
      "Accuracy of Good Movies:  0.13743605921986615\n"
     ]
    }
   ],
   "source": [
    "def cost_of_Bigram(train, test):\n",
    "    match = 0\n",
    "    total = 0\n",
    "    bigramModel = createBigramCount(train)\n",
    "\n",
    "    for sentence in test:\n",
    "        #print(sentence)\n",
    "        index = 0\n",
    "        for word in range(0, len(sentence) - 2):\n",
    "            wordPair = (sentence[index])\n",
    "            if len(bigramModel[sentence[index]]) == 0:\n",
    "                #print(wordPair)\n",
    "                maxToken = ''\n",
    "            else:\n",
    "                maxToken = max(bigramModel[wordPair], key=bigramModel[wordPair].get)\n",
    "            #print(maxToken, sentence[index + 2])\n",
    "            if maxToken == sentence[index + 1]:\n",
    "                match += 1\n",
    "            total += 1\n",
    "            index += 1\n",
    "    return match/total\n",
    "        \n",
    "print(\"Accuracy of Bad Movies: \", cost_of_Bigram(flattenBad, testSet_Bad))\n",
    "print(\"Accuracy of Good Movies: \", cost_of_Bigram(flattenGood, testSet_Good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['oh', ',', 'shit', '.'], ['you', 'just', 'got', 'wolfed', '.'], ['what', '?'], ['that', 'is', 'an', 'official', 'trademark', 'that', 'i', 'am', 'getting', 'registered', '.'], ['it', \"'s\", 'a', 'lot', 'of', 'stuff', 'you', 'got', 'ta', 'do', ',', 'hoops', 'you', 'got', 'ta', 'jump', 'through', '.'], ['got', 'ta', 'get', 'on', 'the', 'internet', '.'], ['got', 'ta', 'go', 'to', 'some', 'stupidass', 'website', 'where', 'you', 'register', 'a', 'catch', 'phrase', '.'], ['i', 'wanted', '``', 'bam', ',', \"''\", 'but', 'emeril', 'had', 'taken', 'it', '.'], ['i', \"'m\", 'rambling', ',', 'man', '.'], ['get', 'up', ',', 'man', '.']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "for sublist in badSentenceList:\n",
    "    del sublist[0]\n",
    "print(badSentenceList[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(badSentenceList, size=200, window=5, min_count=1, workers=4, iter=500)\n",
    "model.save(\"word2vec_Bad_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mr.', 'dufresne', ',', 'describe', 'the', 'confrontation', 'you', 'had', 'with', 'your', 'wife', 'the', 'night', 'she', 'was', 'murdered', '.'], ['it', 'was', 'very', 'bitter', '.'], ['she', 'said', 'she', 'was', 'glad', 'i', 'knew', ',', 'that', 'she', 'hated', 'all', 'the', 'sneaking', 'around', '.'], ['and', 'she', 'said', 'that', 'she', 'wanted', 'a', 'divorce', 'in', 'reno', '.'], ['what', 'was', 'your', 'response', '?'], ['i', 'told', 'her', 'i', 'would', 'not', 'grant', 'one', '.'], ['``', 'i', \"'ll\", 'see', 'you', 'in', 'hell', 'before', 'i', 'see', 'you', 'in', 'reno', '.'], [\"''\", 'those', 'were', 'your', 'words', ',', 'according', 'to', 'your', 'neighbors', '.'], ['if', 'they', 'say', 'so', '.'], ['i', 'really', 'do', \"n't\", 'remember', '.']]\n"
     ]
    }
   ],
   "source": [
    "for sublist in goodSentenceList:\n",
    "    del sublist[0]\n",
    "print(goodSentenceList[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(goodSentenceList, size=200, window=5, min_count=1, workers=4, iter=500)\n",
    "model.save(\"word2vec_Good_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"Tokenized Bad Sentences.csv\", \"w\", newline=\"\", encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(badSentenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hey', ',', 'nate', '.']]\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    your_list = list(reader)\n",
    "    \n",
    "print(your_list[3789:3790])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
